{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d95ebf",
   "metadata": {},
   "source": [
    "# 线性代数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2205e18",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#标量\" data-toc-modified-id=\"标量-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>标量</a></span></li><li><span><a href=\"#向量\" data-toc-modified-id=\"向量-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>向量</a></span><ul class=\"toc-item\"><li><span><a href=\"#长度、纬度和形状\" data-toc-modified-id=\"长度、纬度和形状-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>长度、纬度和形状</a></span></li></ul></li><li><span><a href=\"#矩阵\" data-toc-modified-id=\"矩阵-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>矩阵</a></span></li><li><span><a href=\"#张量\" data-toc-modified-id=\"张量-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>张量</a></span></li><li><span><a href=\"#张量算法的基本性质\" data-toc-modified-id=\"张量算法的基本性质-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>张量算法的基本性质</a></span></li><li><span><a href=\"#降维\" data-toc-modified-id=\"降维-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>降维</a></span><ul class=\"toc-item\"><li><span><a href=\"#非降维求和\" data-toc-modified-id=\"非降维求和-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>非降维求和</a></span></li></ul></li><li><span><a href=\"#点积(Dot-Product)\" data-toc-modified-id=\"点积(Dot-Product)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>点积(Dot Product)</a></span></li><li><span><a href=\"#矩阵-向量积\" data-toc-modified-id=\"矩阵-向量积-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>矩阵-向量积</a></span></li><li><span><a href=\"#矩阵-矩阵乘法\" data-toc-modified-id=\"矩阵-矩阵乘法-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>矩阵-矩阵乘法</a></span></li><li><span><a href=\"#范数\" data-toc-modified-id=\"范数-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>范数</a></span><ul class=\"toc-item\"><li><span><a href=\"#范数和目标\" data-toc-modified-id=\"范数和目标-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>范数和目标</a></span></li></ul></li><li><span><a href=\"#关于线性代数的更多信息\" data-toc-modified-id=\"关于线性代数的更多信息-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>关于线性代数的更多信息</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e11f8d",
   "metadata": {},
   "source": [
    "## 标量\n",
    "采用数学表示法，其中标量变量用小写字母表示（例如： $x、y或z$ ），用$\\mathbb{R}$表示所有连续实数标量的空间，表达式$x\\in\\mathbb{R}$是表示$x$是一个实值标量的正式形式。\n",
    "\n",
    "符号$\\in$称为“属于”，表示“是集合中的成员”。例如$x,y\\in\\{0,1\\}$用来表示$x和y$只能为0或1的数字。\n",
    "\n",
    "标量由只有一个元素的张量表示。下面代码将实例化两个标量，并执行一些算术运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43197b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da12251",
   "metadata": {},
   "source": [
    "## 向量\n",
    "向量可以视作标量组成的列表。这些标量称为向量的元素(element)或分量(component)。当向量表示数据集中的样本时，它们的值具有一定的现实意义。\n",
    "\n",
    "例如，如果我们正在训练⼀个模型来预测贷款违约⻛险，可能会将每个申请⼈与⼀个向量相关联，其分量与其收⼊、⼯作年限、过往违约次数和其他因素相对应。\n",
    "\n",
    "如果我们正在研究医院患者可能⾯临的⼼脏病发作⻛险，可能会⽤⼀个向量来表⽰每个患者，其分量 为最近的⽣命体征、胆固醇⽔平、每天运动时间等。\n",
    "\n",
    "在数学表⽰法中，向量通常记为粗体、⼩写的符号（例如$\\textbf{x、y}和\\textbf{z}$）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f84d7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a999c",
   "metadata": {},
   "source": [
    "我们可以使⽤下标来引⽤向量的任⼀元素，例如可以通过x i 来引⽤第i个元素。注意，元素x i 是⼀个标量，所以 我们在引⽤它时不会加粗。⼤量⽂献认为列向量是向量的默认⽅向，在本书中也是如此。在数学中，向量x可 以写为：\n",
    "\n",
    "$$\\textbf{x} = \\left[\\begin{matrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{matrix}\\right]$$\n",
    "其中$x_1,\\dots,x_n$是向量元素，代码中可以用索引来访问："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51437ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727208ab",
   "metadata": {},
   "source": [
    "### 长度、纬度和形状\n",
    "向量只是⼀个数字数组，就像每个数组都有⼀个⻓度⼀样，每个向量也是如此。\n",
    "\n",
    "在数学表⽰法中，如果我们想说⼀个向量$\\textbf{x}$由n个实值标量组成，可以将其表⽰为$x\\in\\mathbb{R}^n$。\n",
    "\n",
    "向量的⻓度通常称为向量的维度（dimension），与普通的`Python数组`⼀样，我们可以通过调⽤Python的内置`len()`函数来访问张量的⻓度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ca28a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceddd24",
   "metadata": {},
   "source": [
    "当用张量表示一个向量（只有一个轴）时，我们也可以通过`.shape`来属性访问向量的长度。形状（shape）是⼀个元素组，列出了张量沿每个轴的⻓度（维数）。对于只有⼀个轴的张量，形状只有⼀个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12907438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b9edd1",
   "metadata": {},
   "source": [
    "**请注意**，*维度（dimension）* 这个词在不同上下⽂时往往会有不同的含义，这经常会使⼈感到困惑。\n",
    "\n",
    "为了清楚起⻅，我们在此明确⼀下：向量或轴的维度被⽤来表⽰向量或轴的`⻓度`，即向量或轴的元素数量。然⽽，张量的维度⽤来表⽰张量具有的`轴数`。在这个意义上，张量的某个轴的维数就是这个轴的⻓度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da4603",
   "metadata": {},
   "source": [
    "## 矩阵\n",
    "正如向量将标量从零阶推⼴到⼀阶，矩阵将向量从⼀阶推⼴到⼆阶。\n",
    "\n",
    "矩阵，我们通常⽤粗体、⼤写字⺟来表示（例如：$\\textbf{X、Y}$和$\\textbf{Z}$），代码中表示有两个轴的张量。\n",
    "\n",
    "数学表示法使用$\\textbf{A}\\in\\mathbb{R}^{m \\times n}$来表示矩阵$\\textbf{A}$，其由$m$行和$n$列的标量组成。我们可以将任意矩阵$\\textbf{A}\\in\\mathbb{R}^{m \\times n}$视为一个表格，其中每个元素$a_ij$属于第$i$行第$j$列：\n",
    "\n",
    "$$\\textbf{A} = \\left [ \\begin{matrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{matrix} \\right ]$$\n",
    "\n",
    "对于任意矩阵$\\textbf{A}\\in\\mathbb{R}^{m \\times n}$，$\\textbf{A}$的形状是$(m, n)$或$m \\times n$。当矩阵具有相同数量的⾏和列时，其形状将变为正⽅形；因此，它被称为⽅阵（square matrix）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "915238a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(5, 4)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c776af",
   "metadata": {},
   "source": [
    "我们可以通过行索引$(i)$和列索引$(j)$来访问矩阵中的标量元素$a_{ij}$，例如$[\\textbf{A}]_{ij}$。\n",
    "\n",
    "为了表⽰起来简单，只有在必要时才会将逗号插⼊到单独的索引中，例如$a_{2,3j}$和$[\\textbf{A}]_{2i-1,3}$。\n",
    "\n",
    "当我们交换矩阵的⾏和列时，结果称为矩阵的转置（transpose）。通常用$\\textbf{a}^\\top$表示,如果$\\textbf{B}=\\textbf{A}^\\top$，则对于任意的$i和j$,都有$b_{ij}=a_{ji}$。因此表示上文中的$\\textbf{A}$的转置矩阵$(n\\times m)$：\n",
    "\n",
    "$$\\textbf{A}^\\top = \\left [\\begin{matrix}\n",
    "a_{11} & a_{21} & \\cdots & a_{m1} \\\\\n",
    "a_{12} & a_{22} & \\cdots & a_{m2} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{1n} & a_{2n} & \\cdots & a_{mn}\n",
    "\\end{matrix} \\right]$$\n",
    "\n",
    "代码中实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd60cdba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8, 12, 16],\n",
       "        [ 1,  5,  9, 13, 17],\n",
       "        [ 2,  6, 10, 14, 18],\n",
       "        [ 3,  7, 11, 15, 19]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d8573",
   "metadata": {},
   "source": [
    "作为⽅阵的⼀种特殊类型，对称矩阵（symmetric matrix）$\\textbf{A}$等于其转置：$\\textbf{A}=\\textbf{A}^\\top$。定义一个对称矩阵$\\textbf{B}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfcfd55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 0, 4],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d4c0ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B == B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dcf2f5",
   "metadata": {},
   "source": [
    "矩阵是有⽤的数据结构：它们允许我们组织具有不同模式的数据。例如，我们矩阵中的⾏可能对应于不同的房屋（数据样本），⽽列可能对应于不同的属性。\n",
    "\n",
    "因此，尽管单个向量的默认⽅向是列向量，但在表示表格数据集的矩阵中，将每个数据样本作为矩阵中的⾏向量更为常⻅。后⾯的章节将讲到这点，这种约定将⽀持常⻅的深度学习实践。例如，沿着张量的最外轴，我们可以访问或遍历⼩批量的数据样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dbe662",
   "metadata": {},
   "source": [
    "## 张量\n",
    "就像向量是标量的推⼴，矩阵是向量的推⼴⼀样，我们可以构建具有更多轴的数据结构。张量是描述具有任意轴数的$n$维数组的通用方法。\n",
    "\n",
    "张量⽤特殊字体的⼤写字⺟表⽰（例如，$X、Y和Z$），它们的索引机制（例如$x_{ijk}和[X]_{1,2i-1,3}$）与矩阵类似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2abd40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f74a24b",
   "metadata": {},
   "source": [
    "## 张量算法的基本性质\n",
    "标量、向量、矩阵和任意数量轴的张量有一些实用的属性。例如任何按元素的⼀元运算都不会改变其操作数的形状。\n",
    "\n",
    "同样，给定具有相同形状的任意两个张量，任何按元素⼆元运算的结果都将是相同形状的张量。例如，将两个相同形状的矩阵相加，会在这两个矩阵上执⾏元素加法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d22b3ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone() #分配新的内存，A的副本给B\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd0a7c2",
   "metadata": {},
   "source": [
    "具体⽽⾔，两个矩阵的按元素乘法称为Hadamard积（Hadamard product）（数学符号$\\odot$）对于矩阵$\\textbf{B}\\in\\mathbb{R}^{m\\times n}$，其中$i$行$j$列的元素是$b_{ij}$，$\\textbf{A}和\\textbf{B}$的hadamard积为：\n",
    "\n",
    "$$\\textbf{A}⊙\\textbf{B}=\\left[\\begin{matrix}\n",
    "a_{11}b_{11} & a_{12}b_{12} & \\cdots & a_{1n}b_{1n} \\\\\n",
    "a_{21}b_{21} & a_{22}b_{22} & \\cdots & a_{2n}b_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1}b_{m1} & a_{m2}b_{m2} & \\cdots & a_{mn}b_{mn} \\\\\n",
    "\\end{matrix}\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "243adb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e90739",
   "metadata": {},
   "source": [
    "- 广播原则\n",
    "\n",
    "将张量乘以或加上⼀个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c5d9b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a*X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f7e340",
   "metadata": {},
   "source": [
    "## 降维\n",
    "我们可以对任意张量进⾏的⼀个有⽤的操作是计算其元素的和。数学表示法使用$\\sum$符号表示求和。\n",
    "\n",
    "表示长度为d的向量中元素的和，可以记做$\\sum_{i=1}^{d}x_{i}$，代码实现使用求和函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fcb6651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce2b56a",
   "metadata": {},
   "source": [
    "我们可以对任意形状张量求和，例如矩阵$\\textbf{A}$中元素和可以记为$\\sum_{i=1}^{m}\\sum_{j=1}^{n}a_{ij}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc6ee756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), tensor(190.))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6fb76",
   "metadata": {},
   "source": [
    "默认情况下，调⽤求和函数会沿所有的轴降低张量的维度，使它变为⼀个标量。我们还可以指定张量沿哪⼀个轴来通过求和降低维度。\n",
    "\n",
    "以矩阵为例，为了通过求和所有⾏的元素来降维（轴0），可以在调⽤函数时指 定axis=0。由于输⼊矩阵沿0轴降维以⽣成输出向量，因此输⼊轴0的维数在输出形状中消失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "112a7de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([40., 45., 50., 55.]), torch.Size([4]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5e2ddf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f731f38",
   "metadata": {},
   "source": [
    "沿着矩阵行和列求和，等价于对矩阵所有元素求和："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69ede559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(190.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7185030",
   "metadata": {},
   "source": [
    "一个与求和相关的量是平均值（mean,average）。通过元素和除以元素总数来计算平均值，代码中我们可以计算任意形状张量的平均值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39a02c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.5000), tensor(9.5000))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb861261",
   "metadata": {},
   "source": [
    "同样，计算平均值的函数也可以沿指定轴降低张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae4d4653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6faf1e",
   "metadata": {},
   "source": [
    "### 非降维求和\n",
    "但是，有时在调⽤函数来计算总和或均值时保持轴数不变会很有⽤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05b9e1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.],\n",
       "        [22.],\n",
       "        [38.],\n",
       "        [54.],\n",
       "        [70.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5f95b",
   "metadata": {},
   "source": [
    "sum_A求和保持了两个轴，可以通过广播原则A除sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e60412cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
       "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
       "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
       "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
       "        [0.2286, 0.2429, 0.2571, 0.2714]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239bb166",
   "metadata": {},
   "source": [
    "如果我们想沿某个轴计算A元素的累积总和，⽐如axis=0（按⾏计算），可以调⽤cumsum函数。此函数不会沿任何轴降低输⼊张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2df3731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.],\n",
       "        [24., 28., 32., 36.],\n",
       "        [40., 45., 50., 55.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0a3b3",
   "metadata": {},
   "source": [
    "## 点积(Dot Product)\n",
    "\n",
    "学习了按元素操作，求和几平均值。另一个基本操作就是点积。给定两个向量$\\textbf{x,y} \\in \\mathbb{R}^{d}$，它们的点积$\\textbf{x}^\\top\\textbf{y}=$或（$\\left\\langle\\textbf{x,y}\\right\\rangle$）,是相同位置元素乘积的和：$\\textbf{x}^\\top\\textbf{y}=\\sum_{i=1}^{d}{x_{i}y_{i}}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be5323d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(4, dtype=torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5db60",
   "metadata": {},
   "source": [
    "同样可以进行按位相乘然后求和实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8365f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b2f58",
   "metadata": {},
   "source": [
    "点积在很多场合都很有用，例如，给定⼀组由向量$\\textbf{x}\\in\\mathbb{R}^{d}$表示的值，和一组由$\\textbf{w}\\in\\mathbb{R}^{d}$表示的权重。$\\textbf{x}$中的值根据权重$\\textbf{w}$的加权和，可以表⽰为点积$\\textbf{x}^\\top\\textbf{w}$。当权重为⾮负数且和为1(即$\\big(\\sum_{i=1}^{d}\\textbf{w}_{i}=1\\big)$)时，点积表示加权平均。\n",
    "\n",
    "将两个向量规范化后得到单位长度后，点积表⽰它们夹⻆的余弦即($\\mathbf{a\\cdot b}=\\lvert a\\rvert\\lvert b\\rvert\\cos\\theta$)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78403d49",
   "metadata": {},
   "source": [
    "## 矩阵-向量积\n",
    "\n",
    "定义矩阵$\\textbf{A}\\in\\mathbb{R}^{m\\times n}$和向量$\\textbf{x}\\in\\mathbb{R}^{n}$，矩阵$\\textbf{A}$用行向量表示：\n",
    "\n",
    "$$\\textbf{A}=\\left[\\begin{matrix}\n",
    "\\textbf{a}^\\top_1 \\\\\n",
    "\\textbf{a}^\\top_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\textbf{a}^\\top_m \\\\\n",
    "\\end{matrix}\\right]$$\n",
    "\n",
    "其中每个$\\textbf{a}^\\top_i\\in\\mathbb{R}^n$都是行向量，表示矩阵的第$i$行。矩阵向量积$\\textbf{Ax}$是长度为$m$的列向量，其中第$i$个元素是点积$\\textbf{a}_i^\\top\\textbf{x}$:\n",
    "\n",
    "$$\\textbf{Ax}=\\left[\\begin{matrix}\n",
    "\\textbf{a}^\\top_1\\textbf{x} \\\\\n",
    "\\textbf{a}^\\top_2\\textbf{x} \\\\\n",
    "\\vdots \\\\\n",
    "\\textbf{a}^\\top_m\\textbf{x} \\\\\n",
    "\\end{matrix}\\right]$$\n",
    "\n",
    "我们可以把一个矩阵$\\textbf{A}\\in\\mathbb{R}^{m\\times n}$乘法看作是从$\\mathbb{R}^{m\\times n}$到$\\mathbb{R}^{m}$向量的转换。这些转换是⾮常有⽤的，例如可以⽤⽅阵的乘法来表⽰旋转。\n",
    "\n",
    "我们也可以使⽤矩阵-向量积来描述在给定前⼀层的值时，求解神经⽹络每⼀层所需的复杂计算。\n",
    "\n",
    "在代码中使⽤张量表⽰矩阵-向量积，我们使⽤`mv`函数。当我们为矩阵$\\textbf{A}$和向量$\\textbf{x}$调⽤`torch.mv(A, x)`时，会执⾏矩阵-向量积。**注意，A的列维数（沿轴1的⻓度）必须与x的维数（其⻓度）相同。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0df4c06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, x.shape, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e905bf",
   "metadata": {},
   "source": [
    "## 矩阵-矩阵乘法\n",
    "\n",
    "向量积的基础上推广到矩阵乘法比较简单了，假设两个矩阵$\\textbf{A}\\in\\mathbb{R}^{n\\times k}$和$\\textbf{B}\\in\\mathbb{R}^{k\\times m}$:\n",
    "\n",
    "$$\\textbf{A}=\\left[\\begin{matrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{matrix}\\right],\n",
    "\\textbf{B}=\\left[\\begin{matrix}\n",
    "b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    "b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "用行向量$\\textbf{a}_{i}^{\\top}\\in\\mathbb{R}^{k}$表示矩阵$\\textbf{A}$的第$i$行，并让列向量$\\textbf{b}_{j}\\in\\mathbb{R}^{k}$作为矩阵$\\textbf{B}$的第$j$列。生成矩阵$\\textbf{C}=\\textbf{AB}$最简单的方法是考虑$\\textbf{A}$的行向量和$\\textbf{B}$的列向量:\n",
    "\n",
    "$$\n",
    "\\textbf{A}=\\left[\\begin{matrix}\n",
    "a_{1}^{\\top} \\\\\n",
    "a_{2}^{\\top} \\\\\n",
    "\\vdots \\\\\n",
    "a_{n}^{\\top} \\\\\n",
    "\\end{matrix}\\right],\n",
    "\\textbf{B}=\\left[\\begin{matrix}\n",
    "b_{1} & b_{2} & \\cdots & b_{m}\n",
    "\\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "当我们简单地将每个元素$\\textbf{c}_{ij}$计算为点积$\\textbf{a}_i^{\\top}\\textbf{b}_{j}$:\n",
    "\n",
    "$$\n",
    "\\textbf{C}=\\textbf{AB}=\n",
    "\\left[\\begin{matrix}\n",
    "a_{1}^{\\top} \\\\\n",
    "a_{2}^{\\top} \\\\\n",
    "\\vdots \\\\\n",
    "a_{n}^{\\top} \\\\\n",
    "\\end{matrix}\\right]\n",
    "\\left[\\begin{matrix}\n",
    "b_{1} & b_{2} & \\cdots & b_{m}\n",
    "\\end{matrix}\\right]=\n",
    "\\left[\\begin{matrix}\n",
    "\\textbf{a}_{1}^{\\top}\\textbf{b}_{1}&\\textbf{a}_{1}^{\\top}\\textbf{b}_{2}&\\cdots&\\textbf{a}_{1}^{\\top}\\textbf{b}_{m}\\\\\n",
    "\\textbf{a}_{2}^{\\top}\\textbf{b}_{1}&\\textbf{a}_{2}^{\\top}\\textbf{b}_{2}&\\cdots&\\textbf{a}_{2}^{\\top}\\textbf{b}_{m}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\textbf{a}_{n}^{\\top}\\textbf{b}_{1}&\\textbf{a}_{n}^{\\top}\\textbf{b}_{2}&\\cdots&\\textbf{a}_{n}^{\\top}\\textbf{b}_{m}\\\\\n",
    "\\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "我们可以将矩阵-矩阵乘法$\\textbf{AB}$看作简单地执⾏$m$次矩阵-向量积，并将结果拼接在⼀起，形成⼀个$n\\times m$矩阵。\n",
    "\n",
    "在下⾯的代码中，我们在$\\textbf{A}和\\textbf{B}$上执⾏矩阵乘法。这⾥的$\\textbf{A}$是⼀个5⾏4列的矩阵，$\\textbf{B}$是⼀个4⾏3列的矩阵。两者相乘后，我们得到了⼀个5⾏3列的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7591c406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  6.,  6.],\n",
       "        [22., 22., 22.],\n",
       "        [38., 38., 38.],\n",
       "        [54., 54., 54.],\n",
       "        [70., 70., 70.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b6a1f2",
   "metadata": {},
   "source": [
    "矩阵-矩阵乘法可以简单地称为**矩阵乘法**，不应与”Hadamard积”混淆。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f3917",
   "metadata": {},
   "source": [
    "## 范数\n",
    "\n",
    "线性代数中最有⽤的⼀些运算符是范数（norm）。⾮正式地说，向量的范数是表⽰⼀个向量有多⼤。这⾥考虑的⼤⼩（size）概念不涉及维度，⽽是**分量的⼤⼩**。\n",
    "\n",
    "在线性代数中，向量范数是将向量映射到标量的函数$f$。给定任意向量$\\textbf{x}$，向量范数要满⾜⼀些属性。\n",
    "\n",
    "第⼀个性质是：如果我们按常数因⼦$\\alpha$缩放向量的所有元素，其范数也会按相同常数因⼦的绝对值缩放：\n",
    "\n",
    "$$f(\\alpha\\textbf{x})=\\lvert\\alpha\\rvert f(\\textbf{x})$$\n",
    "\n",
    "第⼆个性质是熟悉的三⻆不等式:\n",
    "\n",
    "$$f(\\textbf{x}+\\textbf{y}) \\le f(\\textbf{x}) + f(\\textbf{y})$$\n",
    "\n",
    "第三个性质简单地说范数必须是⾮负的:\n",
    "\n",
    "$$f(\\textbf{x})\\ge 0$$\n",
    "\n",
    "因为在⼤多数情况下，任何东西的最⼩的⼤⼩是0。最后⼀个性质要求范数最⼩为0，当且仅当向量全由0组成。\n",
    "\n",
    "$$\\forall i,[\\textbf{x}]_i = 0 \\Leftrightarrow f(\\textbf{x}) = 0$$\n",
    "\n",
    "范数听起来很像距离的度量。欧⼏⾥得距离和毕达哥拉斯定理中的⾮负性概念和三⻆不等式可能会给出⼀些启发。事实上，欧⼏⾥得距离是⼀个$L_2$范数：假设$n$维向量$\\textbf{x}$中的元素是$x_1, x_2, \\dots, x_n$，其$L_2$范数是向量元素平⽅和的平⽅根：\n",
    "\n",
    "$$\\|\\textbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^{n}{x_i^2}}$$\n",
    "\n",
    "其中，在$L_2$范数中常常省略下标2，也就是说$|\\textbf{x}\\|_2$等同于$|\\textbf{x}\\|$。\n",
    "\n",
    "在代码中，我们可以按如下⽅式计算向量的$L_2$范数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8b1b5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85507391",
   "metadata": {},
   "source": [
    "深度学习中更经常地使⽤$L_2$范数的平⽅，也会经常遇到$L_1$范数，它表⽰为向量元素的绝对值之和：\n",
    "$$\\|\\textbf{x}\\|_1 = \\sum_{i=1}^{n}{\\lvert x_i \\rvert}$$\n",
    "\n",
    "与$L_2$范数相⽐，$L_1$范数受异常值的影响较⼩。为了计算$L_1$范数，我们将绝对值函数和按元素求和组合起来:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0be69b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa88d19",
   "metadata": {},
   "source": [
    "$L_2$范数和$L_1$范数都是更⼀般的$L_p$范数的特例：\n",
    "$$\\|\\textbf{x}\\|_p = \\big(\\sum_{i=1}^{n}{\\lvert x_i \\rvert^p}\\big)^{1/p}$$\n",
    "\n",
    "类似于向量的L 2 范数，矩阵$\\mathbf{X}\\in \\mathbb{R}^{m\\times n}$的*Frobenius*范数（Frobenius norm）是矩阵元素平⽅和的平⽅根：\n",
    "\n",
    "$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m\\sum_{j=1}^n{x_{ij}^2}}$$\n",
    "\n",
    "Frobenius范数满⾜向量范数的所有性质， 它就像是矩阵形向量的$L_2$范数。 调⽤以下函数将计算矩阵的Frobenius范数:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db0475ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones(4, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c91470",
   "metadata": {},
   "source": [
    "### 范数和目标\n",
    "\n",
    "在深度学习中，我们经常试图解决优化问题：最⼤化分配给观测数据的概率; 最⼩化预测和真实观测之间的距离。⽤向量表示物品（如单词、产品或新闻⽂章），以便最⼩化相似项⽬之间的距离，最⼤化不同项⽬之间的距离。⽬标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。\n",
    "\n",
    "## 关于线性代数的更多信息\n",
    "仅⽤⼀节，我们就教会了阅读本书所需的、⽤以理解现代深度学习的线性代数。线性代数还有很多，其中很多数学对于机器学习⾮常有⽤。例如，矩阵可以分解为因⼦，这些分解可以显示真实世界数据集中的低维结构。机器学习的整个⼦领域都侧重于使⽤矩阵分解及其向⾼阶张量的泛化，来发现数据集中的结构并解决预测问题。当开始动⼿尝试并在真实数据集上应⽤了有效的机器学习模型，你会更倾向于学习更多数学。因此， 这⼀节到此结束，本书将在后⾯介绍更多数学知识。 如果渴望了解有关线性代数的更多信息，可以参考[线性代数运算的在线附录38](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html)或其他优秀资源([Kolter, 2008] Kolter, Z. (2008). Linear algebra review and reference. Available online: http.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
